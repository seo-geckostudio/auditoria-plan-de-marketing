#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para analizar completamente el PDF de auditor√≠a SEO
y generar especificaciones detalladas de cada p√°gina
"""

import PyPDF2
import pdfplumber
import re
import json
import os
from pathlib import Path

def analizar_pdf_completo(pdf_path):
    """
    Analiza el PDF completo p√°gina por p√°gina para extraer:
    - Estructura exacta
    - Contenido espec√≠fico
    - Tipos de visualizaci√≥n
    - Fuentes de datos necesarias
    """
    
    especificaciones = {
        "total_paginas": 0,
        "paginas": [],
        "secciones": {},
        "tipos_contenido": {}
    }
    
    print(f"Analizando PDF: {pdf_path}")
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            total_paginas = len(pdf.pages)
            especificaciones["total_paginas"] = total_paginas
            
            print(f"Total de p√°ginas encontradas: {total_paginas}")
            
            for i, page in enumerate(pdf.pages, 1):
                print(f"Procesando p√°gina {i}/{total_paginas}")
                
                # Extraer texto completo
                texto = page.extract_text() or ""
                
                # Extraer tablas
                tablas = page.extract_tables()
                
                # Analizar elementos visuales
                elementos_visuales = analizar_elementos_visuales(page)
                
                # Identificar tipo de contenido
                tipo_contenido = identificar_tipo_contenido(texto, tablas)
                
                # Extraer t√≠tulos y secciones
                titulos = extraer_titulos(texto)
                
                # Determinar fuente de datos
                fuente_datos = determinar_fuente_datos(texto, tipo_contenido)
                
                pagina_spec = {
                    "numero": i,
                    "titulo_principal": titulos.get("principal", ""),
                    "subtitulos": titulos.get("subtitulos", []),
                    "tipo_contenido": tipo_contenido,
                    "texto_completo": texto[:500] + "..." if len(texto) > 500 else texto,
                    "num_tablas": len(tablas) if tablas else 0,
                    "tablas_detalle": analizar_tablas(tablas) if tablas else [],
                    "elementos_visuales": elementos_visuales,
                    "fuente_datos": fuente_datos,
                    "metricas_identificadas": extraer_metricas(texto),
                    "visualizacion_requerida": determinar_visualizacion(texto, tablas, elementos_visuales)
                }
                
                especificaciones["paginas"].append(pagina_spec)
                
                # Agrupar por secciones
                seccion = identificar_seccion(texto, titulos)
                if seccion not in especificaciones["secciones"]:
                    especificaciones["secciones"][seccion] = []
                especificaciones["secciones"][seccion].append(i)
                
                # Contar tipos de contenido
                if tipo_contenido not in especificaciones["tipos_contenido"]:
                    especificaciones["tipos_contenido"][tipo_contenido] = 0
                especificaciones["tipos_contenido"][tipo_contenido] += 1
    
    except Exception as e:
        print(f"Error al procesar PDF: {e}")
        return None
    
    return especificaciones

def analizar_elementos_visuales(page):
    """Analiza elementos visuales en la p√°gina"""
    elementos = {
        "imagenes": 0,
        "graficos": False,
        "charts": False,
        "logos": False,
        "diagramas": False
    }
    
    # Buscar im√°genes
    if hasattr(page, 'images'):
        elementos["imagenes"] = len(page.images)
    
    # Buscar elementos gr√°ficos por texto
    texto = page.extract_text() or ""
    if any(word in texto.lower() for word in ['gr√°fico', 'chart', 'graph', 'diagram']):
        elementos["graficos"] = True
    
    return elementos

def identificar_tipo_contenido(texto, tablas):
    """Identifica el tipo de contenido de la p√°gina"""
    texto_lower = texto.lower()
    
    # Patrones para identificar tipos de contenido
    patrones = {
        "portada": ["auditor√≠a", "seo", "t√©cnica", "informe"],
        "resumen_ejecutivo": ["resumen", "ejecutivo", "summary", "overview"],
        "metricas_trafico": ["sesiones", "usuarios", "pageviews", "tr√°fico", "visitas"],
        "keywords": ["palabras clave", "keywords", "posiciones", "ranking"],
        "analisis_tecnico": ["crawl", "indexaci√≥n", "robots.txt", "sitemap", "404", "301"],
        "velocidad": ["velocidad", "speed", "performance", "carga", "lcp", "fcp"],
        "mobile": ["m√≥vil", "mobile", "responsive", "dispositivos"],
        "contenido": ["contenido", "content", "texto", "h1", "h2", "meta"],
        "enlaces": ["enlaces", "links", "backlinks", "linking", "autoridad"],
        "competencia": ["competencia", "competitors", "competidores"],
        "recomendaciones": ["recomendaciones", "recommendations", "acciones", "mejoras"],
        "graficos": ["gr√°fico", "chart", "datos", "estad√≠sticas"],
        "tablas": ["tabla", "listado", "datos tabulares"]
    }
    
    for tipo, palabras in patrones.items():
        if any(palabra in texto_lower for palabra in palabras):
            if tablas and len(tablas) > 0:
                return f"{tipo}_con_tablas"
            return tipo
    
    if tablas and len(tablas) > 0:
        return "datos_tabulares"
    
    return "contenido_general"

def extraer_titulos(texto):
    """Extrae t√≠tulos principales y subt√≠tulos"""
    lineas = texto.split('\n')
    titulos = {"principal": "", "subtitulos": []}
    
    for linea in lineas[:10]:  # Revisar primeras 10 l√≠neas
        linea = linea.strip()
        if len(linea) > 5 and len(linea) < 100:
            if not titulos["principal"] and (linea.isupper() or any(char.isupper() for char in linea)):
                titulos["principal"] = linea
            elif linea and linea != titulos["principal"]:
                titulos["subtitulos"].append(linea)
    
    return titulos

def determinar_fuente_datos(texto, tipo_contenido):
    """Determina de d√≥nde obtener los datos para replicar el contenido"""
    fuentes = {
        "google_analytics": ["Google Analytics", "GA", "sesiones", "usuarios", "pageviews"],
        "search_console": ["Search Console", "GSC", "impresiones", "clics", "posiciones"],
        "screaming_frog": ["Screaming Frog", "crawl", "spider", "rastreo"],
        "ahrefs": ["Ahrefs", "DR", "UR", "backlinks"],
        "semrush": ["SEMrush", "keywords", "competencia"],
        "pagespeed": ["PageSpeed", "velocidad", "performance", "Core Web Vitals"],
        "manual": ["an√°lisis manual", "revisi√≥n", "inspecci√≥n"],
        "herramientas_seo": ["herramientas SEO", "tools", "an√°lisis t√©cnico"]
    }
    
    texto_lower = texto.lower()
    fuentes_identificadas = []
    
    for fuente, palabras in fuentes.items():
        if any(palabra.lower() in texto_lower for palabra in palabras):
            fuentes_identificadas.append(fuente)
    
    if not fuentes_identificadas:
        # Determinar por tipo de contenido
        mapeo_tipo_fuente = {
            "metricas_trafico": ["google_analytics"],
            "keywords": ["search_console", "ahrefs", "semrush"],
            "analisis_tecnico": ["screaming_frog", "manual"],
            "velocidad": ["pagespeed"],
            "enlaces": ["ahrefs", "semrush"],
            "competencia": ["ahrefs", "semrush"]
        }
        
        for tipo, fuentes_default in mapeo_tipo_fuente.items():
            if tipo in tipo_contenido:
                fuentes_identificadas.extend(fuentes_default)
    
    return fuentes_identificadas if fuentes_identificadas else ["manual"]

def extraer_metricas(texto):
    """Extrae m√©tricas num√©ricas del texto"""
    metricas = []
    
    # Patrones para n√∫meros con contexto
    patrones_metricas = [
        r'(\d{1,3}(?:,\d{3})*)\s*(sesiones|usuarios|pageviews|visitas)',
        r'(\d+(?:\.\d+)?)\s*%\s*(CTR|bounce|conversi√≥n)',
        r'(\d+(?:\.\d+)?)\s*(segundos|seg|s)\s*(carga|velocidad)',
        r'(\d+)\s*(keywords|palabras clave|posiciones)',
        r'(\d+)\s*(enlaces|backlinks|links)',
        r'(\d+)\s*(p√°ginas|URLs|errores)'
    ]
    
    for patron in patrones_metricas:
        matches = re.finditer(patron, texto, re.IGNORECASE)
        for match in matches:
            metricas.append({
                "valor": match.group(1),
                "tipo": match.group(2),
                "contexto": match.group(0)
            })
    
    return metricas

def analizar_tablas(tablas):
    """Analiza las tablas encontradas"""
    tablas_detalle = []
    
    for i, tabla in enumerate(tablas):
        if tabla and len(tabla) > 0:
            detalle = {
                "numero": i + 1,
                "filas": len(tabla),
                "columnas": len(tabla[0]) if tabla[0] else 0,
                "headers": tabla[0] if tabla[0] else [],
                "tipo_datos": identificar_tipo_datos_tabla(tabla),
                "muestra_datos": tabla[:3] if len(tabla) > 1 else tabla
            }
            tablas_detalle.append(detalle)
    
    return tablas_detalle

def identificar_tipo_datos_tabla(tabla):
    """Identifica el tipo de datos en la tabla"""
    if not tabla or len(tabla) < 2:
        return "desconocido"
    
    headers = [str(cell).lower() if cell else "" for cell in tabla[0]]
    
    if any(word in " ".join(headers) for word in ["keyword", "palabra", "posici√≥n", "ranking"]):
        return "keywords"
    elif any(word in " ".join(headers) for word in ["url", "p√°gina", "status", "c√≥digo"]):
        return "urls_tecnico"
    elif any(word in " ".join(headers) for word in ["sesiones", "usuarios", "tr√°fico"]):
        return "metricas_trafico"
    elif any(word in " ".join(headers) for word in ["enlace", "link", "dominio"]):
        return "enlaces"
    else:
        return "datos_generales"

def determinar_visualizacion(texto, tablas, elementos_visuales):
    """Determina qu√© tipo de visualizaci√≥n se necesita"""
    visualizaciones = []
    
    # Basado en tablas
    if tablas:
        for tabla in tablas:
            if tabla and len(tabla) > 1:
                visualizaciones.append("tabla_html")
    
    # Basado en elementos visuales
    if elementos_visuales.get("graficos"):
        visualizaciones.append("grafico_chart_js")
    
    # Basado en contenido textual
    texto_lower = texto.lower()
    if any(word in texto_lower for word in ["gr√°fico", "chart", "estad√≠stica"]):
        visualizaciones.append("grafico_interactivo")
    
    if any(word in texto_lower for word in ["lista", "listado", "elementos"]):
        visualizaciones.append("lista_html")
    
    if not visualizaciones:
        visualizaciones.append("texto_formateado")
    
    return visualizaciones

def identificar_seccion(texto, titulos):
    """Identifica a qu√© secci√≥n pertenece la p√°gina"""
    texto_completo = (titulos.get("principal", "") + " " + " ".join(titulos.get("subtitulos", [])) + " " + texto).lower()
    
    secciones = {
        "introduccion": ["portada", "√≠ndice", "introducci√≥n", "overview"],
        "resumen_ejecutivo": ["resumen", "ejecutivo", "summary", "conclusiones"],
        "analisis_actual": ["situaci√≥n actual", "an√°lisis", "estado", "diagn√≥stico"],
        "metricas_rendimiento": ["rendimiento", "m√©tricas", "kpis", "performance"],
        "analisis_tecnico": ["t√©cnico", "crawl", "indexaci√≥n", "estructura"],
        "contenido_seo": ["contenido", "seo", "optimizaci√≥n", "keywords"],
        "experiencia_usuario": ["usuario", "ux", "usabilidad", "m√≥vil"],
        "competencia": ["competencia", "competitors", "benchmark"],
        "recomendaciones": ["recomendaciones", "acciones", "plan", "mejoras"],
        "anexos": ["anexo", "ap√©ndice", "datos", "detalles"]
    }
    
    for seccion, palabras in secciones.items():
        if any(palabra in texto_completo for palabra in palabras):
            return seccion
    
    return "general"

def generar_documento_especificaciones(especificaciones, output_path):
    """Genera el documento de especificaciones en Markdown"""
    
    contenido = f"""# üìã ESPECIFICACIONES DETALLADAS - AUDITOR√çA SEO WEB
## Replicaci√≥n del PDF Original en HTML para Impresi√≥n A4 Apaisado

### üìä RESUMEN GENERAL
- **Total de p√°ginas**: {especificaciones['total_paginas']}
- **Secciones identificadas**: {len(especificaciones['secciones'])}
- **Tipos de contenido**: {len(especificaciones['tipos_contenido'])}

### üóÇÔ∏è DISTRIBUCI√ìN POR SECCIONES
"""
    
    for seccion, paginas in especificaciones['secciones'].items():
        contenido += f"- **{seccion.replace('_', ' ').title()}**: P√°ginas {', '.join(map(str, paginas))} ({len(paginas)} p√°ginas)\n"
    
    contenido += f"""

### üìà TIPOS DE CONTENIDO IDENTIFICADOS
"""
    
    for tipo, cantidad in especificaciones['tipos_contenido'].items():
        contenido += f"- **{tipo.replace('_', ' ').title()}**: {cantidad} p√°ginas\n"
    
    contenido += f"""

---

## üìÑ ESPECIFICACIONES DETALLADAS POR P√ÅGINA

"""
    
    for pagina in especificaciones['paginas']:
        contenido += f"""
### üìÑ P√ÅGINA {pagina['numero']}

**üè∑Ô∏è T√≠tulo Principal**: {pagina['titulo_principal'] or 'Sin t√≠tulo identificado'}

**üìÇ Secci√≥n**: {identificar_seccion_pagina(pagina['numero'], especificaciones['secciones'])}

**üéØ Tipo de Contenido**: {pagina['tipo_contenido'].replace('_', ' ').title()}

**üìù Subt√≠tulos**:
{chr(10).join([f"- {sub}" for sub in pagina['subtitulos']]) if pagina['subtitulos'] else "- No se identificaron subt√≠tulos"}

**üìä Contenido Identificado**:
- **Texto**: {'S√≠' if pagina['texto_completo'] else 'No'}
- **Tablas**: {pagina['num_tablas']} tabla(s)
- **Elementos visuales**: {'S√≠' if any(pagina['elementos_visuales'].values()) else 'No'}

**üîç M√©tricas Identificadas**:
{chr(10).join([f"- {m['contexto']}" for m in pagina['metricas_identificadas']]) if pagina['metricas_identificadas'] else "- No se identificaron m√©tricas espec√≠ficas"}

**üìã Detalles de Tablas**:
"""
        
        if pagina['tablas_detalle']:
            for tabla in pagina['tablas_detalle']:
                contenido += f"""
- **Tabla {tabla['numero']}**: {tabla['filas']} filas √ó {tabla['columnas']} columnas
  - **Tipo**: {tabla['tipo_datos'].replace('_', ' ').title()}
  - **Headers**: {', '.join([str(h) for h in tabla['headers']]) if tabla['headers'] else 'Sin headers'}
"""
        else:
            contenido += "- No contiene tablas\n"
        
        contenido += f"""
**üé® Visualizaci√≥n Requerida**:
{chr(10).join([f"- {vis.replace('_', ' ').title()}" for vis in pagina['visualizacion_requerida']])}

**üì° Fuente de Datos**:
{chr(10).join([f"- {fuente.replace('_', ' ').title()}" for fuente in pagina['fuente_datos']])}

**üíª Implementaci√≥n HTML**:
- **Layout**: A4 Apaisado (297mm √ó 210mm)
- **CSS Print**: `@media print` espec√≠fico
- **Elementos**: {'Tablas responsivas, ' if pagina['num_tablas'] > 0 else ''}{'Gr√°ficos Chart.js, ' if any('grafico' in vis for vis in pagina['visualizacion_requerida']) else ''}Texto formateado

**üîÑ Proceso de Obtenci√≥n de Datos**:
"""
        
        # Generar proceso espec√≠fico seg√∫n fuente de datos
        for fuente in pagina['fuente_datos']:
            if fuente == 'google_analytics':
                contenido += "- Conectar con Google Analytics API para obtener m√©tricas de tr√°fico\n"
            elif fuente == 'search_console':
                contenido += "- Extraer datos de Google Search Console (impresiones, clics, posiciones)\n"
            elif fuente == 'screaming_frog':
                contenido += "- Ejecutar crawl con Screaming Frog y procesar resultados\n"
            elif fuente == 'ahrefs':
                contenido += "- Consultar API de Ahrefs para m√©tricas de enlaces y autoridad\n"
            elif fuente == 'semrush':
                contenido += "- Obtener datos de SEMrush para an√°lisis de keywords y competencia\n"
            elif fuente == 'pagespeed':
                contenido += "- Ejecutar PageSpeed Insights API para m√©tricas de velocidad\n"
            elif fuente == 'manual':
                contenido += "- An√°lisis manual y recopilaci√≥n de datos espec√≠ficos del sitio\n"
            else:
                contenido += f"- Proceso personalizado para {fuente.replace('_', ' ')}\n"
        
        contenido += f"""
**üìù Muestra de Contenido**:
```
{pagina['texto_completo'][:200]}...
```

---
"""
    
    contenido += f"""

## üõ†Ô∏è GU√çA DE IMPLEMENTACI√ìN T√âCNICA

### üì± Estructura HTML Base
```html
<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Auditor√≠a SEO - P√°gina {{numero}}</title>
    <link rel="stylesheet" href="styles/print.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body class="a4-landscape">
    <div class="page-container">
        <!-- Contenido espec√≠fico de cada p√°gina -->
    </div>
</body>
</html>
```

### üé® CSS para Impresi√≥n A4 Apaisado
```css
@media print {{
    .a4-landscape {{
        width: 297mm;
        height: 210mm;
        margin: 0;
        padding: 20mm;
        page-break-after: always;
    }}
    
    .page-container {{
        width: 100%;
        height: 100%;
        display: flex;
        flex-direction: column;
    }}
}}
```

### üìä Integraci√≥n de Datos
- **APIs necesarias**: Google Analytics, Search Console, PageSpeed Insights
- **Herramientas**: Screaming Frog, Ahrefs/SEMrush (seg√∫n disponibilidad)
- **Procesamiento**: Scripts Python para extracci√≥n y formateo de datos
- **Actualizaci√≥n**: Sistema automatizado para regenerar p√°ginas con datos actuales

### üîÑ Flujo de Trabajo
1. **Extracci√≥n de datos** de todas las fuentes identificadas
2. **Procesamiento y formateo** seg√∫n especificaciones de cada p√°gina
3. **Generaci√≥n HTML** con datos actualizados de Ibiza Villa
4. **Aplicaci√≥n de estilos** para impresi√≥n A4 apaisado
5. **Validaci√≥n** de contenido y formato
6. **Exportaci√≥n** a PDF final si es necesario

---

## üìã CHECKLIST DE IMPLEMENTACI√ìN

### ‚úÖ Preparaci√≥n
- [ ] Configurar acceso a todas las APIs necesarias
- [ ] Instalar y configurar herramientas de an√°lisis
- [ ] Preparar estructura de archivos HTML/CSS
- [ ] Configurar scripts de extracci√≥n de datos

### ‚úÖ Desarrollo por Secciones
"""
    
    for seccion, paginas in especificaciones['secciones'].items():
        contenido += f"- [ ] **{seccion.replace('_', ' ').title()}**: P√°ginas {', '.join(map(str, paginas))}\n"
    
    contenido += f"""
### ‚úÖ Validaci√≥n y Testing
- [ ] Verificar formato A4 apaisado en todas las p√°ginas
- [ ] Comprobar legibilidad de tablas y gr√°ficos
- [ ] Validar datos actualizados de Ibiza Villa
- [ ] Testing de impresi√≥n en diferentes navegadores
- [ ] Revisi√≥n final de contenido y formato

---

*Documento generado autom√°ticamente el {__import__('datetime').datetime.now().strftime('%d/%m/%Y %H:%M')}*
*Total de p√°ginas analizadas: {especificaciones['total_paginas']}*
"""
    
    # Guardar el documento
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(contenido)
    
    print(f"Documento de especificaciones guardado en: {output_path}")

def identificar_seccion_pagina(numero_pagina, secciones):
    """Identifica a qu√© secci√≥n pertenece una p√°gina espec√≠fica"""
    for seccion, paginas in secciones.items():
        if numero_pagina in paginas:
            return seccion.replace('_', ' ').title()
    return "General"

def main():
    # Rutas de archivos
    pdf_path = r"c:\ai\investigacionauditoria programacion\FASE_5_ENTREGABLES_FINALES\auditoria seo tecnica.pdf"
    output_path = r"c:\ai\investigacionauditoria programacion\ESPECIFICACIONES_113_PAGINAS_AUDITORIA_WEB.md"
    json_output = r"c:\ai\investigacionauditoria programacion\especificaciones_detalladas.json"
    
    print("üîç Iniciando an√°lisis completo del PDF de auditor√≠a SEO...")
    
    # Verificar que existe el PDF
    if not os.path.exists(pdf_path):
        print(f"‚ùå Error: No se encuentra el archivo PDF en {pdf_path}")
        return
    
    # Analizar PDF
    especificaciones = analizar_pdf_completo(pdf_path)
    
    if especificaciones:
        print(f"‚úÖ An√°lisis completado. {especificaciones['total_paginas']} p√°ginas procesadas.")
        
        # Guardar especificaciones en JSON
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump(especificaciones, f, ensure_ascii=False, indent=2)
        
        # Generar documento de especificaciones
        generar_documento_especificaciones(especificaciones, output_path)
        
        print(f"üìÑ Documento de especificaciones generado: {output_path}")
        print(f"üíæ Datos detallados guardados en: {json_output}")
        
    else:
        print("‚ùå Error al analizar el PDF")

if __name__ == "__main__":
    main()